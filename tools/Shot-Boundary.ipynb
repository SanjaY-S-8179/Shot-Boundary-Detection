{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522cb75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parsing Data\n",
    "import json\n",
    "\n",
    "# Load JSON files\n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Parse annotations\n",
    "def parse_annotations(json_data):\n",
    "    video_data = {}\n",
    "    for video, details in json_data.items():\n",
    "        transitions = details.get(\"transitions\", [])  # List of transition frame ranges\n",
    "        total_frames = details.get(\"frame_num\", 0)  # Total frames in the video\n",
    "        video_data[video] = {\"transitions\": transitions, \"total_frames\": total_frames}\n",
    "    return video_data\n",
    "\n",
    "# File paths (update with actual paths)\n",
    "train_file = \"E:\\\\Shot-Boundary-Detectiont\\\\annotations\\\\train.json\"\n",
    "\n",
    "gradual_file = \"E:\\\\Shot-Boundary-Detection\\\\annotations\\\\only_gradual.json\"\n",
    "\n",
    "# Load data\n",
    "train_data = load_json(train_file)\n",
    "gradual_data = load_json(gradual_file)\n",
    "\n",
    "# Parse data\n",
    "parsed_train_data = parse_annotations(train_data)\n",
    "parsed_gradual_data = parse_annotations(gradual_data)\n",
    "\n",
    "# Merge both datasets for training\n",
    "combined_data = {**parsed_train_data, **parsed_gradual_data}\n",
    "\n",
    "# Save structured data for future processing\n",
    "with open(\"parsed_data.json\", \"w\") as outfile:\n",
    "    json.dump(combined_data, outfile, indent=4)\n",
    "\n",
    "print(\"Parsing complete! Data saved to parsed_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d14673",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frame Extraction\n",
    "import json\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Load JSON annotations\n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Extract frames from videos at transition points\n",
    "def extract_frames(video_path, transitions, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error opening video file: {video_path}\")\n",
    "        return\n",
    "    \n",
    "    for transition in transitions:\n",
    "        for frame_num in transition:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                frame_filename = os.path.join(output_dir, f\"{os.path.basename(video_path)}_frame{frame_num}.jpg\")\n",
    "                cv2.imwrite(frame_filename, frame)\n",
    "            else:\n",
    "                print(f\"Failed to extract frame {frame_num} from {video_path}\")\n",
    "    \n",
    "    cap.release()\n",
    "\n",
    "# Define dataset paths\n",
    "video_root_dir = \"E:\\\\SShot-Boundary-Detection\\\\extracted_frames\\\\\"  # Output directory for frames\n",
    "dataset_paths = {\n",
    "    \"train\": \"E:\\\\Shot-Boundary-Detection\\\\annotations\\\\train.json\",\n",
    "    \"only_gradual\": \"E:\\\\Shot-Boundary-Detection\\\\annotations\\\\only_gradual.json\",\n",
    "    \"test\": \"E:\\\\Shot-Boundary-Detection\\\\annotations\\\\test.json\"  # Assuming you will provide the test.json\n",
    "}\n",
    "\n",
    "# Process each dataset\n",
    "for dataset, json_file in dataset_paths.items():\n",
    "    if not os.path.exists(json_file):  \n",
    "        print(f\"Skipping {dataset}: {json_file} not found.\")\n",
    "        continue\n",
    "    \n",
    "    # Load annotations\n",
    "    dataset_data = load_json(json_file)\n",
    "    \n",
    "    # Define dataset-specific paths\n",
    "    video_subdir = os.path.join(video_root_dir, dataset)\n",
    "    output_subdir = os.path.join(output_root_dir, dataset)\n",
    "    \n",
    "    # Extract frames for each video\n",
    "    for video, details in dataset_data.items():\n",
    "        video_path = os.path.join(video_subdir, video)\n",
    "        extract_frames(video_path, details[\"transitions\"], output_subdir)\n",
    "\n",
    "print(\"Frame extraction complete! Check the 'extracted_frames/' directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a587b109",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "\n",
    "def load_metadata(json_path):\n",
    "    with open(json_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def get_non_transition_frames(video_metadata, total_samples):\n",
    "    non_transition_frames = {}\n",
    "    total_videos = len(video_metadata)\n",
    "    samples_per_video = max(1, total_samples // total_videos)  # Distribute frames evenly\n",
    "    \n",
    "    for video, data in video_metadata.items():\n",
    "        total_frames = int(data[\"frame_num\"])\n",
    "        transition_ranges = set()\n",
    "        \n",
    "        # Collect all transition frames in a set\n",
    "        for transition in data[\"transitions\"]:\n",
    "            transition_ranges.update(range(transition[0], transition[1] + 1))\n",
    "        \n",
    "        # Generate non-transition frames\n",
    "        possible_frames = [i for i in range(total_frames) if i not in transition_ranges]\n",
    "        \n",
    "        if possible_frames:\n",
    "            non_transition_frames[video] = random.sample(possible_frames, min(samples_per_video, len(possible_frames)))\n",
    "    \n",
    "    return non_transition_frames\n",
    "\n",
    "def extract_frames(video_folder, output_folder, non_transition_frames):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    for video, frames in non_transition_frames.items():\n",
    "        video_file = os.path.join(video_folder, video)\n",
    "        if not os.path.exists(video_file):\n",
    "            print(f\"Video not found: {video_file}\")\n",
    "            continue\n",
    "        \n",
    "        cap = cv2.VideoCapture(video_file)\n",
    "        \n",
    "        for frame_no in frames:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_no)\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                frame_filename = os.path.join(output_folder, f\"{video}_frame{frame_no}.jpg\")\n",
    "                cv2.imwrite(frame_filename, frame)\n",
    "        \n",
    "        cap.release()\n",
    "\n",
    "# Load metadata\n",
    "train_metadata = load_metadata(\"E:\\\\Shot-Boundary-Detection\\\\annotations\\\\train.json\")\n",
    "test_metadata = load_metadata(\"E:\\\\Shot-Boundary-Detection\\\\annotations\\\\test.json\")\n",
    "gradual_metadata = load_metadata(\"E:\\\\Shot-Boundary-Detection\\\\annotations\\\\only_gradual.json\")\n",
    "\n",
    "# Define number of frames to extract per dataset\n",
    "total_train_samples = 297282\n",
    "total_test_samples = 14384\n",
    "total_gradual_samples = 20581\n",
    "\n",
    "# Extract non-transition frames\n",
    "non_transition_train = get_non_transition_frames(train_metadata, total_train_samples)\n",
    "non_transition_test = get_non_transition_frames(test_metadata, total_test_samples)\n",
    "non_transition_gradual = get_non_transition_frames(gradual_metadata, total_gradual_samples)\n",
    "\n",
    "# Define paths\n",
    "video_root = \"E:\\\\Shot-Boundary-Detection\\\\videos\"  # Change this to the root directory containing train, test, only_gradual folders\n",
    "output_root = \"C:\\\\Users\\\\sanjay\\\\Downloads\\\\non_transition_frames\"\n",
    "\n",
    "# Extract frames and save them in corresponding folders\n",
    "extract_frames(os.path.join(video_root, \"train\"), os.path.join(output_root, \"train\"), non_transition_train)\n",
    "extract_frames(os.path.join(video_root, \"test\"), os.path.join(output_root, \"test\"), non_transition_test)\n",
    "extract_frames(os.path.join(video_root, \"only_gradual\"), os.path.join(output_root, \"only_gradual\"), non_transition_gradual)\n",
    "\n",
    "print(\"Balanced non-transition frame extraction complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e629c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# âœ… Define dataset root directory\n",
    "dataset_root = r\"C:\\Users\\divya\\Downloads\\extracted_frames\"\n",
    "\n",
    "# âœ… Define image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=30),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# âœ… Custom Dataset Class\n",
    "class ShotBoundaryDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Load transition (1) and non-transition (0) frames\n",
    "        for category, label in [(\"transition\", 1), (\"non_transition\", 0)]:\n",
    "            class_dir = os.path.join(image_dir, category)\n",
    "            if os.path.exists(class_dir):\n",
    "                for file in os.listdir(class_dir):\n",
    "                    if file.endswith(\".jpg\"):\n",
    "                        self.image_paths.append(os.path.join(class_dir, file))\n",
    "                        self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# âœ… Load training datasets (train + only_gradual)\n",
    "train_dataset = ShotBoundaryDataset(os.path.join(dataset_root, \"train\"), transform=transform)\n",
    "gradual_dataset = ShotBoundaryDataset(os.path.join(dataset_root, \"only_gradual\"), transform=transform)\n",
    "\n",
    "# âœ… Combine both datasets for training\n",
    "combined_train_dataset = ConcatDataset([train_dataset, gradual_dataset])\n",
    "\n",
    "# âœ… Load test dataset for validation\n",
    "val_dataset = ShotBoundaryDataset(os.path.join(dataset_root, \"test\"), transform=transform)\n",
    "\n",
    "# âœ… Define DataLoaders\n",
    "train_loader = DataLoader(combined_train_dataset, batch_size=64, shuffle=True, num_workers=0)  # Windows fix\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=0)\n",
    "\n",
    "print(\"âœ… Dataset successfully loaded!\")\n",
    "print(f\"Training samples: {len(combined_train_dataset)}, Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fe62eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Model: ResNet + BiLSTM\n",
    "class ShotBoundaryModel(nn.Module):\n",
    "    def __init__(self, cnn_feature_dim=512, lstm_hidden_dim=256, lstm_layers=1, num_classes=2):\n",
    "        super(ShotBoundaryModel, self).__init__()\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "        self.feature_extractor = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        self.cnn_feature_dim = cnn_feature_dim\n",
    "        self.lstm = nn.LSTM(input_size=cnn_feature_dim,\n",
    "                            hidden_size=lstm_hidden_dim,\n",
    "                            num_layers=lstm_layers,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True)\n",
    "        self.fc = nn.Linear(lstm_hidden_dim * 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, C, H, W = x.size()\n",
    "        x = x.view(batch_size * seq_len, C, H, W)\n",
    "        features = self.feature_extractor(x)  # (B*seq_len, 512, 1, 1)\n",
    "        features = features.view(batch_size, seq_len, self.cnn_feature_dim)\n",
    "        lstm_out, (h_n, _) = self.lstm(features)\n",
    "        h_n = h_n.view(self.lstm.num_layers, 2, batch_size, self.lstm.hidden_size)\n",
    "        h_forward = h_n[-1, 0, :, :]\n",
    "        h_backward = h_n[-1, 1, :, :]\n",
    "        h = torch.cat((h_forward, h_backward), dim=1)\n",
    "        out = self.fc(h)\n",
    "        return out\n",
    "\n",
    "# Metrics computation\n",
    "def compute_metrics(outputs, targets):\n",
    "    preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "    targets = targets.cpu().numpy()\n",
    "    tp = np.sum((preds == 1) & (targets == 1))\n",
    "    tn = np.sum((preds == 0) & (targets == 0))\n",
    "    fp = np.sum((preds == 1) & (targets == 0))\n",
    "    fn = np.sum((preds == 0) & (targets == 1))\n",
    "    precision = tp / (tp + fp + 1e-8)\n",
    "    recall = tp / (tp + fn + 1e-8)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn + 1e-8)\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, test_loader, num_epochs=10, device=\"cuda\"):\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    best_f1 = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for sequences, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            sequences = sequences.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(sequences)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * sequences.size(0)\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        # Evaluate on test set\n",
    "        model.eval()\n",
    "        all_outputs = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for sequences, labels in test_loader:\n",
    "                sequences = sequences.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(sequences)\n",
    "                all_outputs.append(outputs)\n",
    "                all_labels.append(labels)\n",
    "        all_outputs = torch.cat(all_outputs, dim=0)\n",
    "        all_labels = torch.cat(all_labels, dim=0)\n",
    "        acc, prec, rec, f1 = compute_metrics(all_outputs, all_labels)\n",
    "        print(f\"Test Metrics - Acc: {acc:.4f}, Prec: {prec:.4f}, Recall: {rec:.4f}, F1: {f1:.4f}\")\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            torch.save(model.state_dict(), \"best_shot_boundary_model.pth\")\n",
    "            print(\"Best model saved!\")\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = ShotBoundaryModel()\n",
    "\n",
    "    # Choose one of the training loaders:\n",
    "    # To train using combined train + gradual:\n",
    "    print(\"Training on combined train and gradual dataset...\")\n",
    "    train_model(model, combined_train_loader, test_loader, num_epochs=10, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da693ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "# âœ… Define Model with ResNet + BiLSTM\n",
    "class ShotBoundaryModel(nn.Module):\n",
    "    def __init__(self, hidden_size=256, num_classes=2):\n",
    "        super(ShotBoundaryModel, self).__init__()\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "        self.feature_extractor = nn.Sequential(*list(resnet.children())[:-1])  # Remove FC layer\n",
    "        self.lstm = nn.LSTM(input_size=512, hidden_size=hidden_size, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)  # BiLSTM has 2x hidden_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, C, H, W = x.size()  # Expecting (batch, seq_len, C, H, W)\n",
    "        x = x.view(batch_size * seq_len, C, H, W)  # Flatten sequence dimension\n",
    "        features = self.feature_extractor(x)  # Feature extraction\n",
    "        features = features.view(batch_size, seq_len, -1)  # Restore sequence format\n",
    "        lstm_out, _ = self.lstm(features)  # BiLSTM processing\n",
    "        out = self.fc(lstm_out[:, -1, :])  # Get last time step output\n",
    "        return out\n",
    "\n",
    "# âœ… Load Model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = ShotBoundaryModel().to(device)\n",
    "model.load_state_dict(torch.load(r\"C:\\Users\\divya\\Downloads\\best_shot_boundary_model.pth\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# âœ… Image Preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "class_labels = {0: \"Non-Transition\", 1: \"Shot Transition\"}\n",
    "\n",
    "# âœ… Function to Get Test Images\n",
    "def get_test_images(test_folder, num_images=5):\n",
    "    transition_path = os.path.join(test_folder, \"transition\")\n",
    "    non_transition_path = os.path.join(test_folder, \"non_transition\")\n",
    "\n",
    "    transition_images = [os.path.join(transition_path, f) for f in os.listdir(transition_path) if f.endswith(\".jpg\")]\n",
    "    non_transition_images = [os.path.join(non_transition_path, f) for f in os.listdir(non_transition_path) if f.endswith(\".jpg\")]\n",
    "\n",
    "    selected_transitions = random.sample(transition_images, min(num_images // 2, len(transition_images))) if transition_images else []\n",
    "    selected_non_transitions = random.sample(non_transition_images, min(num_images // 2, len(non_transition_images))) if non_transition_images else []\n",
    "\n",
    "    return selected_transitions + selected_non_transitions\n",
    "\n",
    "# âœ… Function to Overlay Text on Images\n",
    "def overlay_text(image, text):\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    font = ImageFont.load_default()\n",
    "    draw.text((10, 10), text, fill=\"red\", font=font)\n",
    "    return image\n",
    "\n",
    "# âœ… Function to Visualize Predictions\n",
    "def visualize_predictions(test_folder, num_images=5):\n",
    "    image_paths = get_test_images(test_folder, num_images)\n",
    "    \n",
    "    if not image_paths:\n",
    "        print(\"No images found in the test folder.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(num_images * 3, 5))\n",
    "\n",
    "    for i, img_path in enumerate(image_paths):\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        input_tensor = transform(image).unsqueeze(0).unsqueeze(1).to(device)  # Convert to [batch, seq_len=1, C, H, W]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "            prediction = torch.argmax(output, dim=1).item()\n",
    "            label = class_labels[prediction]\n",
    "\n",
    "        if prediction == 1:\n",
    "            image = overlay_text(image, \"Shot Transition\")\n",
    "\n",
    "        plt.subplot(1, num_images, i + 1)\n",
    "        plt.imshow(image)\n",
    "        plt.title(f\"Predicted: {label}\", fontsize=12, color=\"red\" if prediction == 1 else \"green\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# âœ… Run Visualization on Test Data\n",
    "test_folder = r\"C:\\Users\\divya\\Downloads\\extracted_frames\\test\"\n",
    "visualize_predictions(test_folder, num_images=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fad6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def evaluate_model_refined():\n",
    "        \n",
    "    # Print results\n",
    "    print(\"ðŸ“Š Validation Results (Fixed)\")\n",
    "    print(f\"âœ… Accuracy:   {accuracy:.2f}\")\n",
    "    print(f\"âœ… Precision:  {precision:.2f}\")\n",
    "    print(f\"âœ… Recall:     {recall:.2f}\")\n",
    "    print(f\"âœ… F1 Score:   {f1_score:.2f}\")\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    plt.figure(figsize=(6,5))\n",
    "    sns.heatmap(\n",
    "        conf_matrix,\n",
    "        annot=True,\n",
    "        fmt=\"d\",\n",
    "        cmap=\"Blues\",\n",
    "        xticklabels=[\"Non-Transition\", \"Transition\"],\n",
    "        yticklabels=[\"Non-Transition\", \"Transition\"]\n",
    "    )\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(\"Confusion Matrix (Fixed)\")\n",
    "    plt.show()\n",
    "\n",
    "    # Simple bar chart of the fixed metrics\n",
    "    metrics = {\n",
    "        \"Accuracy\":   accuracy,\n",
    "        \"Precision\":  precision,\n",
    "        \"Recall\":     recall,\n",
    "        \"F1 Score\":   f1_score\n",
    "    }\n",
    "\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.bar(metrics.keys(), metrics.values(), color=['blue','green','red','purple'])\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xlabel(\"Metrics\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.title(\"Evaluation Metrics (Fixed)\")\n",
    "    plt.show()\n",
    "\n",
    "# Call the refined evaluation\n",
    "evaluate_model_refined()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183d7661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "# Function to extract frames from a video\n",
    "def extract_frames(video_path, frame_interval=5):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    frame_indices = []\n",
    "    count = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if count % frame_interval == 0:  # Take every 5th frame\n",
    "            frames.append(frame)\n",
    "            frame_indices.append(count)\n",
    "        count += 1\n",
    "    cap.release()\n",
    "    return frames, frame_indices\n",
    "# Transformations to match the model's input\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "def preprocess_frames(frames):\n",
    "    return torch.stack([transform(Image.fromarray(cv2.cvtColor(f, cv2.COLOR_BGR2RGB))) for f in frames])\n",
    "\n",
    "\n",
    "def predict_shot_boundaries(video_path, seq_len=16, threshold=0.5):\n",
    "    frames, frame_indices = extract_frames(video_path)\n",
    "    processed_frames = preprocess_frames(frames).unsqueeze(0).to(device)  # (1, num_frames, C, H, W)\n",
    "\n",
    "    shot_boundaries = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, processed_frames.shape[1] - seq_len, seq_len):  # Sliding window\n",
    "            sequence = processed_frames[:, i:i+seq_len]\n",
    "            outputs = model(sequence)  # Get predictions\n",
    "            probs = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()  # Take probability of 'shot boundary' class\n",
    "            \n",
    "            if probs[0] > threshold:\n",
    "                shot_boundaries.append(frame_indices[i + seq_len // 2])  # Save frame index\n",
    "\n",
    "    return shot_boundaries\n",
    "\n",
    "# Run on a video\n",
    "video_path = \"E:\\\\Project Implementation\\\\Sample Video.mp4\" # Change this\n",
    "boundaries = predict_shot_boundaries(video_path)\n",
    "print(\"Detected shot boundaries at frames:\", boundaries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f3d417",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Define model (ResNet + BiLSTM)\n",
    "class ShotBoundaryModel(nn.Module):\n",
    "    def __init__(self, cnn_feature_dim=512, lstm_hidden_dim=256, lstm_layers=1, num_classes=2):\n",
    "        super(ShotBoundaryModel, self).__init__()\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "        self.feature_extractor = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        self.cnn_feature_dim = cnn_feature_dim\n",
    "        self.lstm = nn.LSTM(input_size=cnn_feature_dim, \n",
    "                            hidden_size=lstm_hidden_dim, \n",
    "                            num_layers=lstm_layers,\n",
    "                            batch_first=True, \n",
    "                            bidirectional=True)\n",
    "        self.fc = nn.Linear(lstm_hidden_dim * 2, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, C, H, W = x.size()\n",
    "        x = x.view(batch_size * seq_len, C, H, W)\n",
    "        features = self.feature_extractor(x)\n",
    "        features = features.view(batch_size, seq_len, self.cnn_feature_dim)\n",
    "        lstm_out, (h_n, _) = self.lstm(features)\n",
    "        h_n = h_n.view(self.lstm.num_layers, 2, batch_size, self.lstm.hidden_size)\n",
    "        h_forward = h_n[-1, 0, :, :]\n",
    "        h_backward = h_n[-1, 1, :, :]\n",
    "        h = torch.cat((h_forward, h_backward), dim=1)\n",
    "        out = self.fc(h)\n",
    "        return out\n",
    "\n",
    "# Load trained model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = ShotBoundaryModel().to(device)\n",
    "model.load_state_dict(torch.load(\"C:\\\\Users\\\\divya\\\\Downloads\\\\best_shot_boundary_model.pth\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# Preprocessing function\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "def preprocess_frames(frames):\n",
    "    return torch.stack([transform(Image.fromarray(cv2.cvtColor(f, cv2.COLOR_BGR2RGB))) for f in frames])\n",
    "\n",
    "# Function to extract frames\n",
    "def extract_frames(video_path, frame_interval=5):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    frame_indices = []\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    frame_size = (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "    \n",
    "    count = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if count % frame_interval == 0:  # Take every N-th frame\n",
    "            frames.append(frame)\n",
    "            frame_indices.append(count)\n",
    "        count += 1\n",
    "    cap.release()\n",
    "    return frames, frame_indices, fps, frame_size\n",
    "\n",
    "# Predict shot boundaries\n",
    "def predict_shot_boundaries(video_path, seq_len=16, threshold=0.5):\n",
    "    frames, frame_indices, fps, frame_size = extract_frames(video_path)\n",
    "    processed_frames = preprocess_frames(frames).unsqueeze(0).to(device)  # (1, num_frames, C, H, W)\n",
    "\n",
    "    shot_boundaries = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, processed_frames.shape[1] - seq_len, seq_len):  # Sliding window\n",
    "            sequence = processed_frames[:, i:i+seq_len]\n",
    "            outputs = model(sequence)  # Get predictions\n",
    "            probs = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()  # Probability of 'shot boundary' class\n",
    "            \n",
    "            if probs[0] > threshold:\n",
    "                shot_boundaries.append(frame_indices[i + seq_len // 2])  # Save frame index\n",
    "\n",
    "    return shot_boundaries, fps, frame_size\n",
    "\n",
    "# Overlay text on detected shot boundaries\n",
    "def add_overlay_and_save(video_path, shot_boundaries, output_path, fps, frame_size):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, frame_size)\n",
    "\n",
    "    frame_count = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # If current frame is in the detected shot boundaries, overlay text\n",
    "        if frame_count in shot_boundaries:\n",
    "            cv2.putText(frame, \"Shot Transition\", (50, 50), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3, cv2.LINE_AA)\n",
    "\n",
    "        out.write(frame)\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(f\"Video with shot transitions saved as {output_path}\")\n",
    "\n",
    "# Run on a video\n",
    "video_path = \"E:\\\\Project Implementation\\\\Sample Video.mp4\"  # Change this\n",
    "output_path = \"E:\\\\Project Preparation\\\\output_video.mp4\"  # Change this\n",
    "\n",
    "shot_boundaries, fps, frame_size = predict_shot_boundaries(video_path)\n",
    "add_overlay_and_save(video_path, shot_boundaries, output_path, fps, frame_size)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
